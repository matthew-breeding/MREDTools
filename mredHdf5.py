#!/labs/isde/mred/mred-20181015/bin/python

## This helper modules provides methods for interacting the MRED's HDF5 tools. The two main
##	functional tools here are the hdf5RunManager which governs incremental saving of HDF5
## 	files and is the preferred method of running MRED in both beamOn mode and single evennt
## 	mode. The second tool is the smart accumulate tool which is intended to be used in 
## 	conjunction with the hdf5Cleanup.py script (saved in ~/.defaults). This allows multiple
## 	HDF5 files from parallelized runs to be intelligently combined based on histogram name
## 	with the output being only one HDF5 file with all data combined. 
## 	Additionally, the getXY method is defined here which returns a list of (x,y) pairs for 
## 	given histogram object iff the y values are non-zero in that bin.
import os, shutil, tarfile

def tarFile(fileToTar, tarName):
    with tarfile.open(tarName, "w:gz") as tar:
	tar.add(fileToTar, arcname = os.path.basename(fileToTar))

## compresses and removes the most recent valid .hdf5 file. Used in hdf5RunManager
def tarFileForRunManager(hdf5File, options):
    """
    This method compresses and routes the output data files forom each incremental beamOn() 
	run (or singleEventMode run) in hdf5RunManager(). The options namespace is an input
	to direct the output file save directory. 

    For a given @hdf5File (which is the full path string -- e.g. /scratch/userName/data/neutronRun.hdf5), 
	the .hdf5 file is compressed to a .tar.gz file, a check is run to 
    """
    outputFilename = hdf5File.split(".hdf5")[0] + ".tar.gz"

    tarFile(hdf5File, outputFilename)

    if os.path.isfile(outputFilename) and os.path.isfile(hdf5File):
	os.remove(hdf5File)

    tarIndex = int(outputFilename.split(".tar.gz")[0].split("_")[-1])
    oldTarName = "{}_{}.tar.gz".format('_'.join(outputFilename.split(".tar.gz")[0].split("_")[:-1]), str(tarIndex - 1))
    
    # check if there is are two tar files, removes the old one if not retaining all 
    if os.path.isfile(outputFilename) and os.path.isfile(oldTarName):
	os.remove(oldTarName)

    # Check if using a /tmp directory; if so, move the newly tar'd file to the options.saveDir (on /home or /scratch)
    if outputFilename[:5] == '/tmp/':
	shutil.move(outputFilename, options.saveDir + os.path.basename(outputFilename))  # copy the tar file to the current directory 

    # If not retaining all files, remove the previous tar file from the save directory 
    if not options.retainAll:
	if os.path.isfile(options.saveDir + os.path.basename(outputFilename)) and os.path.isfile(options.saveDir + os.path.basename(oldTarName)):
	    os.remove(options.saveDir + os.path.basename(oldTarName))



##  Merges the options strings with any custom **kwargs options, to be included
##  in the Hdf5 attributes upon save
def constructAttributes(options, **kwargs):
    attrsDict = {}
    for k, v in kwargs.items():
	attrsDict.update({k:v})
    attrsDict.update(options.__dict__)
    return attrsDict


def hdf5RunManager(mred, options, bv=None, singleEventMode = False, function=None, saveDose = False, **kwargs):
    """
    This method sets up batch runs to incrementally save hdf5 files at regular 
	intervals of nIons for both beamOn mode and single event mode. 

    The mred and options instances are REQUIRED to be passed when calling the 
	function; optionally, the @batch_vars class can be passed for jobs submitted
	to a compute node. If there is an instance of batch_vars, the run index for 
	each file name is procedurally generated by adding the options.index and the
	batch_vars.index. This is useful for parallelized runs that are not 
	necessarily submitted at the same time. 

    The options namespace MUST include the following attributes: 
	options.nIons     -->  the TOTAL number of particles to be simulated for the 
							entire run
	options.nSaves    -->  number of individual save files; i.e. for nIons = 1000 
							and nSaves = 10, hdf5RunManager splits the simulation 
							into 10 beamOn() commands each with 100 particles 
							simulated with a new data output file created for each
							of the 10 branches --> 10 total output files, each with 
							data for 100 particles* (*IMPORTANT -- see note in 
							options.retainAll for essential info)
	options.retainAll -->  default value is False; for histograms that are auto-
							generated in MRED, (mred.autogenerate_histograms = True
							in the main script) the histogram values are reset for 
							each mred.beamOn() command. However, if the histograms 
							are not autogenerated but instead manually created and 
							attached to the sensitive volumes/detectors, the 
							histogram tables in HDF5 files are automatically updated
							across all mred.beamOn() commands. This means that you 
							would only end up with one output data file instead of 
							10 incremental files that would still need to be 
							combined in post-processing.  
							It is better practice in general to explicitly name 
							your sensitive regions and explicitly define your 
							histogram parameters. An explicit catch is built in to 
							automatically toggle this flag based on the 
							mred.autogenerate_histograms attribute. 
	options.runName   -->  self explanatory
	options.saveDir   -->  final save directory for data output. Must be valid 
							path in /home or /scratch
	options.index     -->  the run index. For single runs, this is just 0; for 
							multiple simulations with the same initial conditions, 
							this can be incremented to facilitate post-processing 
							combinations.

    The HDF5 file attributes that are handled automatically here are the file_path, 
	file_name,and setting write_outputs = True. Every other mred.hdf5 option is left 
	to the user to select within the main script  ** include mred.hdf5.include_
	histograms (which defaults to False) **.  File names are incremented for each 
	mred.beamOn() command (or each mred.runSingleEventMode). 

    Any **kwargs passed to the function are also automatically added to HDF5 output
	file's attributes table.  This is especially useful if there are certain values
	of interest that don't require loading all of the histograms into memory.

    At the end of each incremental run, the tarFile() method handles the file output
	behavior. For runs saving to /tmp/, the .hdf5 file is compressed and moved to the
	options.saveDir directory, with the files on /tmp getting removed. If already saving 
	at options.saveDir, the old .hdf5 files are removed after compression and the old 
	.tar.gz files are removed only if options.retainAll == False (default behavior -- 
	see note above)
    """
    from datetime import datetime
    nIons= 0

    try:
	options.batchIndex = bv.index
	options.batchRunName = bv.runName
    except:
	options.batchIndex = 0
	options.batchRunName = options.runName

    hdf5Name ="{}{:03}".format(options.runName, options.batchIndex + options.index)
    mred.hdf5.file_name = hdf5Name+'_0.hdf5'

    ## Look for the path created using my custom mredBatch script for saving; if not there, set the 
    ##  	save directory to options.saveDir
    if os.path.exists("/tmp/mred_{}{:03d}/".format(options.batchRunName, options.batchIndex)):
	mred.hdf5.file_path = "/tmp/mred_{}{:03d}/".format(options.batchRunName, options.batchIndex)
    else:
	mred.hdf5.file_path = options.saveDir
    mred.hdf5.write_output_files = True

    # set up default file attributes to save (default attributes include the options strings and any **kwargs)
    attributesDict = constructAttributes(options, **kwargs)

    # manually make sure these two attrs are saved.
    attributesDict.update({'gfu':mred.gun.fluence_unit})
    attributesDict.update({'nIons':0}) 	

    starttime=datetime.now()
    mred.progress_interval = options.nIons/options.nSaves/10
    for i in range(options.nSaves):

        nIons += options.nIons/options.nSaves
	attributesDict.update({'nIons': nIons, 'runTime':mred.up_time})
	if saveDose:
	    attributesDict.update({h.name + "XY_Total": h.xy_total for h in mred.histogram_list })
	    
	## Print status string 
        print '\n'*3 + '-'*30
        print 'beginning run %d with %d particles this run of %d total particles in this batch'%((i+1), float(nIons)/options.nSaves, options.nIons)
        print "total number of histograms: %d"%len(mred.histogram_list)
	print "total run time to now: "
	print datetime.now() - starttime
        print '-'*30 + '\n'*3

	#  Run the simulation 
        if singleEventMode:
            mred.runSingleEventMode(options.nIons/options.nSaves, function=function)
        else:
            mred.beamOn(options.nIons/options.nSaves)

	# explicitly check for autogenerated histograms
	if mred.autogenerate_histograms and not options.retainAll: 
	    print "\n"*20 + "*"*100
	    print("WARNING: mred.autogenerate_histograms is set to TRUE\n \
Incremental saving only works for explicitly generated histograms. \
\n For autogenerated histograms, the bins are reset for each beamOn command, \
partiailly defeating the purpose of this method.\n \
Setting options.retainAll --> True and continuing the run ...")
	    print "*"*100 + "\n"*20
	    options.retainAll= True

	# set file attributes
        try:
	    for k, v in attributesDict.items():
		mred.hdf5.setFileAttribute(k, v)
        except Exception, e:
            print 'ERROR setting file attributes'
            print e

	# Set new Hdf5 file name, incrementing the index by 1
	mred.hdf5.file_name = hdf5Name +'_'+ str(i+1) + '.hdf5'

	# Tar the old Hdf5 file, remove the older tar and hdf5 files
	hdf5ToTar = mred.hdf5.file_path + hdf5Name + '_' + str(i) + '.hdf5'
	tarFileForRunManager(hdf5ToTar, options)


### Returns a list of (x,y) tuples only for histograms with y values > 0
def getXY(histogram):
    hist = [(x[0], x[1]) for x in histogram.xy_pairs if x[1] > 0]
    return hist

# accepts list of files, the name of the attribute to check, and the type of check (defaults to same)
# 	set test variable to None to return the a list of the attribute values for each run. 
def checkAttrs(filesIn, attr, test='same'):
    import glob as _glob
    import mred_hdf5 as _hdf
    attrs =[]
    if type(filesIn) == str:
	nameList = _glob.glob(filesIn)
    elif type(filesIn) == list:
	nameList = filesIn

    for f in nameList:
	run = _hdf.openMred(f)
	attrs.append(run.root._v_attrs.__getattribute__(attr))
	run.close()

    if test == 'same':
	if attrs.count(attrs[0]) == len(attrs):
	    return True
	else:
	    return False
    else:
	return [a.item() for a in attrs]

# returns a dictionary with the file attribute names and their values for a given file. 
def getAttrDict(fileIn):
    import mred_hdf5 as _hdf

    attrDict = {}
    run = _hdf.openMred(fileIn)
    attrs  =  run.root._v_attrs._v_attrnames
    for a in attrs:
	attrDict[a] = run.root._v_attrs.__getattribute__(a)
    run.close()
    return attrDict
    
# Takes either a list of files or an expression for files in a directory (i.e. outputData/*). Checks to make sure the gun
# 	fluence unit is the same between runs by default, then combines histograms according to the histogram name.
# 	Returns list of histograms. This is a significant improvement over the mred_hdf5.accumulate method as it allows
# 	for combination of hdf5 files where the histograms were not necessarily assigned in the same order or in single 
#	event mode when histograms are dynamically created and populated resulting in an uneven number of histograms between runs. 
# 	NOTE: it is still required that histograms of the same name between runs be compatible. This will NOT catch the 
#	error of two histograms with the same names but different bins / logspacing rules. 
def smartAccumulate(filesIn, checkGFU=True):
    import mred_hdf5 as _hdf
    import glob as _glob
    import os as _os

    if type(filesIn) == str:
	nameList = _glob.glob(filesIn)
    elif type(filesIn) == list:
	nameList = filesIn
    
    newNameList = []
    for n in nameList:
	try:
	    f = _hdf.openMred(n)
	    newNameList.append(n)
	    f.close()
	    print("Adding {} to the list of run files".format(n))
	except:
	    _os.remove(n)
	    print 'ERROR: couldnt open this file, removing from list: %s'%n
    nameList = newNameList
    if len(nameList) == 0:
	return False

    if checkGFU:
	attrs = checkAttrs(nameList, 'gfu')
	if not attrs:
	    print '\n'*10+'!'*50
	    print 'ERROR: gun fluences do not match!'
	    print '!'*50+'\n'*10

    print filesIn
    histograms = _hdf.getHistograms(filesIn)
    histogramNames = {}
    if type(histograms[0]) == list:
	for run in histograms:
	    for h in run:
		if histogramNames.has_key(h.name):
		    histogramNames[h.name] += h
		else:
		    histogramNames[h.name] = h
#	added 05 Nov 2019 after nyc flux problems... maybe not necessary?
    elif type(histograms[0] == None):
	print 'no histogram here....weird'
	print histograms
    else:
	for h in histograms:
	    if histogramNames.has_key(h.name):
		histogramNames[h.name] += h
	    else:
		histogramNames[h.name] = h

    return histogramNames.values()
