# MREDTools

A collection of modules to help MRED users


parsers.py
==================

The parsers module instantiates common command line option flags used in MRED. 

Most common usage:

```
import parsers
options, extra = parsers.defaultOptions()
```

To add custom flags, select the method based on type. 

```
parsers.addStrOption("customFlag", "defaultValue")
options, extra = parsers.updateOptions()
```

(This was done for efficiency, and I still need to more completely wrap the Argparse package -- low priority though since it works as-is)

The `defaultOptions()` method loads the following flags:

option flag &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| default value | description
--- | --- | ---
--particle | neutron | defines the particle beam species
--beamType | directionalFlux | defines the particle beam flux option. Options include directionalFlux, pointSource, isotropicFlux, and others
--waferMat | silicon | defines the material of the wafer 
--runName | defaultRunName20200317.234513 |  defines the run name
--saveDir | <currentDirectory>/outputData/ |the directory path for saving output data from the run
--beamE | 1. | defines the particle beam energy in MeV. Defaults to 1 MeV
--beamTilt | 0. | defines the particle beam tilt (polar angle) in degrees
--beamRoll | 0. | defines the particle beam roll (azimuthal angle)  in degrees
--rangeCuts | 1.0 | range cuts (in um). Defines the minimum threshold energy (using range for secondary production)
--beamA | 1 | defines the particle beam atomic mass | 
--beamZ | 1 | defines the particle beam atomic number
--nIons | 100 | sets the total number of particles to run |
--index | 0 | index offset value, used to increment the batch index when saving files |
--maxSteps | 20000 | maximum number of steps in a given event  |
--sBias | 500 | Hadronic cross section biasing factor |
--nSaves | 10 | number of incremental progress saves |
--retainAll | False | Keep all of the files generated as incremental saves. Useful for retaining the steps/tracks in HDF5 |
--dx | False | Use the OpenDx view for event-by-event viewing |
--suv | False | Use the Geant4 OpenGL Viewer  |

mredHdf5.py
====================

This module contains methods for interacting with the HDF5 fomratted output files from MRED both during run time and in post-processing. Some of the methods are used in the hdf5Cleanup.py module (see below). 

`hdf5RunManager()` is the most commonly used method. The use example is: 

```
from mredHdf5 import hdf5RunManager
"""
(all of the MRED definitions & setup)
"""
hdf5RunManager(mred, options)
```

To run in single event mode: 

`hdf5RunManager(mred,options, singleEventMode = True, function = doEventFunction)`

To run when submitting jobs to a cluster, you must include the `batch_vars` argument (`mred` and `options` must always be the first two arguments passed):

`hdf5RunManager(mred, options, batch_vars)`

Ideally this is used in conjunction with the `parsers` module, as certain options flags are expected in the `options` Namespace passed to the method. Also, if you wish to utilize file saving on `/tmp` for compute node jobs, use `mredBatch.py` (see below)

From the comments in `mredHdf5.py`: 

```
  """
     This method sets up batch runs to incrementally save hdf5 files at regular 
     intervals of nIons for both beamOn mode and single event mode. 
 
     The mred and options instances are REQUIRED to be passed when calling the 
     function; optionally, the @batch_vars class can be passed for jobs submitted
     to a compute node. If there is an instance of batch_vars, the run index for 
     each file name is procedurally generated by adding the options.index and the
     batch_vars.index. This is useful for parallelized runs that are not 
     necessarily submitted at the same time. 

     The options namespace MUST include the following attributes: 
     options.nIons     -->  the TOTAL number of particles to be simulated for the 
                             entire run
     options.nSaves    -->  number of individual save files; i.e. for nIons = 1000 
                             and nSaves = 10, hdf5RunManager splits the simulation 
                             into 10 beamOn() commands each with 100 particles 
                             simulated with a new data output file created for each
                             of the 10 branches --> 10 total output files, each with 
                             data for 100 particles* (*IMPORTANT -- see note in 
                             options.retainAll for essential info)
     options.retainAll -->  default value is False; for histograms that are auto-
                             generated in MRED, (mred.autogenerate_histograms = True
                             in the main script) the histogram values are reset for 
                             each mred.beamOn() command. However, if the histograms 
                             are not autogenerated but instead manually created and 
                             attached to the sensitive volumes/detectors, the 
                             histogram tables in HDF5 files are automatically updated
                             across all mred.beamOn() commands. This means that you 
                             would only end up with one output data file instead of 
                              10 incremental files that would still need to be                                                                                                                                      
                             combined in post-processing.  
                             It is better practice in general to explicitly name 
                             your sensitive regions and explicitly define your 
                             histogram parameters. An explicit catch is built in to 
                             automatically toggle this flag based on the 
                             mred.autogenerate_histograms attribute. 
     options.runName   -->  self explanatory
     options.saveDir   -->  final save directory for data output. Must be valid 
                             path in /home or /scratch
     options.index     -->  the run index. For single runs, this is just 0; for 
                             multiple simulations with the same initial conditions, 
                             this can be incremented to facilitate post-processing 
                             combinations.
 
     The HDF5 file attributes that are handled automatically here are the file_path, 
     file_name,and setting write_outputs = True. Every other mred.hdf5 option is left 
     to the user to select within the main script  ** include mred.hdf5.include_
     histograms (which defaults to False) **.  File names are incremented for each 
     mred.beamOn() command (or each mred.runSingleEventMode). 
 
     Any **kwargs passed to the function are also automatically added to HDF5 output
     file's attributes table.  This is especially useful if there are certain values
     of interest that don't require loading all of the histograms into memory.
 
     At the end of each incremental run, the tarFile() method handles the file output
     behavior. For runs saving to /tmp/, the .hdf5 file is compressed and moved to the
     options.saveDir directory, with the files on /tmp getting removed. If already saving 
     at options.saveDir, the old .hdf5 files are removed after compression and the old 
     .tar.gz files are removed only if options.retainAll == False (default behavior -- 
     see note above)
     """
```

hdf5Cleanup.py
============

This is a python executable that collects all of the .hdf5 files in the current directory and combines all of the histograms across those files into one combined HDF5 output file. The file is saved by default in a directory called `combinedHDF5output`, with a file name of the format `xxxxxxTOTAL.hdf5`. This using MRED, so on ACCRE machines you must `module load mred` prior to use.

The output file name and directory can be explicitly specified at the command line using the flags `--saveName` and `--saveDir` respectively. 

Additionally, if you have HDF5 files from several different runs in the same directory, you can specify which files to combine using the `--files` flag (e.g. `./hdf5Cleanup.py --files=silicon*.hdf5`)

**Note that this will combine histograms solely based on their names -- it is the user's responsibility to ensure cross-compatible naming conventions between parallelized run output files**


mredBatch.py
===============

This executable is used to setup batch submissions on the cluster. It is 95% the old mred_batch code, with a couple of modifications for creating the directories for sorting default output files such as `killFiles`. An example batch submission would look like this: 

`~/path/to/mredBatch.py --runName=simulationRunName --rumTime=86400 --nCopies=10 mred -f --init simulationScript.py --saveDir=/path/to/saveDir/ --nIons=1000000 --nSaves=10  ## etc... with run option flags`

It is oftentimes more efficient to put a bunch of these commands in one file and submit all at once. 
